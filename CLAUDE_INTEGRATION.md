# Claude API Integration

This document explains how to use Claude models alongside Ollama models in the LLM Logger.

## Setup

### 1. Install Required Packages

```bash
pip install -r requirements.txt
```

### 2. Configure API Key

Get your Claude API key from: https://console.anthropic.com/settings/keys

**Recommended Method (using .env file):**

```bash
# Copy the example file
cp .env.example .env

# Edit .env and replace 'your_api_key_here' with your actual key
# The file should look like:
# ANTHROPIC_API_KEY=sk-ant-api03-xxxxxxxxxxxxx
```

**Alternative (environment variable):**

```bash
export ANTHROPIC_API_KEY="sk-ant-api03-xxxxxxxxxxxxx"
```

⚠️ **Important**: Never commit your `.env` file with real API keys! The `.env` file is already in `.gitignore` to prevent this.

### 3. Run the Application

```bash
python local_llm_logger_v3_continuous_chat.py
```

Open http://127.0.0.1:5005 in your browser.

## Features

### Available Claude Models

The following Claude models are available in the model dropdown:

- **claude-opus-4-20250514** - Most capable model, best for complex tasks
- **claude-3-5-sonnet-20241022** - Balanced performance and speed
- **claude-3-5-haiku-20241022** - Fastest model, best for simple tasks

### Token Tracking

The system automatically tracks token usage for all models:

- **Input tokens**: Tokens sent to the model (prompt + context)
- **Output tokens**: Tokens generated by the model (response)
- **Total tokens**: Combined input + output

Token statistics are displayed in the UI as a chip in the top-right corner.

### Rate Limiting

Claude has two types of rate limits:

1. **5-Hour Rolling Window**: Primary limit that resets every 5 hours
2. **Weekly Limit**: Secondary cap for heavy users

The system estimates token usage to help you track your consumption.

### Model Switching

You can switch between Ollama and Claude models at any time:

- Models starting with `claude-` use the Claude API
- All other models use Ollama (local)

The system automatically routes requests to the appropriate backend.

### Unified Logging

All conversations are logged identically regardless of model:

- CSV exports (conversations.csv, turns.csv)
- JSONL files (per turn)
- Markdown files
- HTML files
- DOCX files (if pypandoc is installed)

Token statistics are included in the turn data for Claude models.

## File Context

Files uploaded to a conversation are automatically included in the context for both Ollama and Claude models. This is particularly useful for:

- Code review
- Document analysis
- Data processing tasks

## YouTube Content Creation

The comprehensive logging makes it easy to create content:

- All prompts and responses are saved
- Response times tracked for performance comparisons
- Token usage tracked for cost analysis
- Multiple export formats for editing tools

## Troubleshooting

### "ANTHROPIC_API_KEY environment variable not set"

Make sure you've set the API key:

```bash
export ANTHROPIC_API_KEY="your_key_here"
```

### "Claude support not available"

Install the required packages:

```bash
pip install anthropic tiktoken
```

### Models not showing in dropdown

Check that:
1. The API key is set correctly
2. The packages are installed
3. The Flask server has been restarted

## Cost Monitoring

Token tracking helps you monitor API costs:

- Claude Opus 4.1: $20/M input tokens, $80/M output tokens
- Claude Sonnet 3.5: $5/M input tokens, $25/M output tokens
- Claude Haiku 3.5: Lowest cost option

The UI displays total tokens used in thousands (e.g., "5.2k tokens").

## Next Steps

- Add rate limit warnings when approaching limits
- Implement automatic conversation summarization
- Add cost calculation based on current pricing
- Support for additional Claude models as they're released
